# -*- coding: utf-8 -*-
#InvertedIndex uses a table-lookup method, storing the stems along with the index in a Python dictionary
#In our case is word_index() that consist of word_split(), normalization and the deletion of stop words ("the", "then", "that"...).


#stop words list is generated by a predefined list of stop words which read and converted into a list


stopWordsFile = open('stopwords.txt', 'r')
stopWordsList = [line.split(',') for line in stopWordsFile.readlines()]

_STOP_WORDS = stopWordsList

_WORD_MIN_LENGTH = 1


#stemmer

suffixes = {
    1: ["ो", "े", "ू", "ु", "ी", "ि", "ा"],
    2: ["कर", "ाओ", "िए", "ाई", "ाए", "ने", "नी", "ना", "ते", "ीं", "ती", "ता", "ाँ", "ां", "ों", "ें"],
    3: ["ाकर", "ाइए", "ाईं", "ाया", "ेगी", "ेगा", "ोगी", "ोगे", "ाने", "ाना", "ाते", "ाती", "ाता", "तीं", "ाओं", "ाएं", "ुओं", "ुएं", "ुआं"],
    4: ["ाएगी", "ाएगा", "ाओगी", "ाओगे", "एंगी", "ेंगी", "एंगे", "ेंगे", "ूंगी", "ूंगा", "ातीं", "नाओं", "नाएं", "ताओं", "ताएं", "ियाँ", "ियों", "ियां"],
    5: ["ाएंगी", "ाएंगे", "ाऊंगी", "ाऊंगा", "ाइयाँ", "ाइयों", "ाइयां"],
}


def hi_stem(word):
    for L in 5, 4, 3, 2, 1:
        if len(word) > L + 1:
            for suf in suffixes[L]:
                if word.endswith(suf.decode('utf-8')):
                    return word[:-L]
    return word



#end of stemmer


def word_split(text):
    word_list = []
    wcurrent = []
    windex = None

    for i, c in enumerate(text):
        if c.isalnum():
            wcurrent.append(c)
            windex = i
        elif wcurrent:
            word = u''.join(wcurrent)
            word_list.append((windex - len(word) + 1, word))
            wcurrent = []

    if wcurrent:
        word = u''.join(wcurrent)
        word_list.append((windex - len(word) + 1, word))

    return word_list

#cleaning and normalizing the words

#elemenating the words from the word list which are a part of stop words


def words_cleanup(words):
    cleaned_words = []
    for index, word in words:
        if len(word) < _WORD_MIN_LENGTH or word in _STOP_WORDS:
            continue
        hi_stem(word)
        cleaned_words.append((index, word))
    return cleaned_words


def words_normalize(words):
    normalized_words = []
    for index, word in words:
        wnormalized = word.lower()
        normalized_words.append((index, wnormalized))
    return normalized_words


def word_index(text):
    words = word_split(text)
    words = words_normalize(words)
    words = words_cleanup(words)
    return words


def inverted_index(text):
    inverted = {}

    for index, word in word_index(text):
        locations = inverted.setdefault(word, [])
        locations.append(index)

    return inverted






#The function below takes a Multi-Document Inverted index and a query, and returns the set of documents that contains all the words that you've searched.


def inverted_index_add(inverted, doc_id, doc_index):
    for word, locations in doc_index.iteritems():
        indices = inverted.setdefault(word, {})
        indices[doc_id] = locations
    return inverted


if __name__ == '__main__':
    import sys
    if len(sys.argv) != 1:
        sys.exit('{} takes no arguments'.format(sys.argv[0]))
    #for line in sys.stdin:
        #print([hi_stem(word) for word in line.split()])

    # inverted text file

    str = open('test.txt', 'r').read()

    print inverted_index(str.decode('utf-8'))




